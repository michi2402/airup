import json
import os.path
import random
from collections import defaultdict
from datasets import Dataset, load_from_disk
from sklearn.model_selection import train_test_split

from reranking.config import *
from reranking.question_clustering import get_similar_questions

def get_all_questions(filename):
    """Loads questions and their associated snippets from a JSON file."""
    with open(filename) as f:
        data = json.load(f)

    questions = []
    questions_with_snippets = []
    for i, q in enumerate(data["questions"]):
        questions.append(q["body"])
        snippets = q.get("snippets", [])
        questions_with_snippets.append(snippets)

    return questions, questions_with_snippets

def save_data_from_file(filename):
    """
        Loads question and snippet data from a JSON file, generates positive and
        negative training samples for reranking, splits the data into training
        and test sets, and saves them to disk in HuggingFace Dataset format.

        Positive samples are derived directly from labeled snippets in the dataset.
        Negative samples are generated by retrieving similar questions and randomly
        sampling snippets from them, with different label values depending on
        similarity closeness.
    """
    with open(filename) as f:
        data = json.load(f)

    question_groups = defaultdict(list)

    #positive samples
    for q in data["questions"]:
        question = q["body"]
        snippets = q.get("snippets", [])
        for snippet in snippets:
            question_groups[question].append({
                "question": question,
                "snippet": snippet["text"],
                "label": float(1),
                "document": snippet["document"],
            })

    #negative samples
    all_questions, questions_with_snippets = get_all_questions(filename)
    similar_question_groups = get_similar_questions(all_questions)
    for i, similar_qs in enumerate(similar_question_groups):
        question = all_questions[i]
        for j, q_index in enumerate(similar_qs):
            random_snippet = random.choice(questions_with_snippets[q_index])
            label = LABEL_CLOSE_NEG if j < AMOUNT_SAME_SIMILAR else LABEL_MID_NEG if j < AMOUNT_MID_SIMILAR else LABEL_FAR_NEG
            question_groups[question].append({
                "question": question,
                "snippet": random_snippet["text"],
                "label": float(label),
                "document": random_snippet["document"],
            })

    train_questions, test_questions  = train_test_split(all_questions, test_size=0.1)

    train_dataset = Dataset.from_list([row for q in train_questions for row in question_groups[q]])
    test_dataset = Dataset.from_list([row for q in test_questions for row in question_groups[q]])

    filename_for_save = filename[len(DATA_FOLDER):].split(".")[0]
    train_dataset.save_to_disk(filename_for_save + TRAIN_FILE_EXTENSION)
    test_dataset.save_to_disk(filename_for_save + TEST_FILE_EXTENSION)
    return test_dataset, all_questions

def get_training_data(filename):
    """
        Loads or generates a training dataset from a given data file.

        Checks whether a preprocessed training dataset already exists on disk.
        If it does, it loads the dataset from disk. If not it calls a fnc to preprocess and save it.
    """
    filename_for_save = filename[len(DATA_FOLDER):].split(".")[0]
    if os.path.exists(filename_for_save +  TRAIN_FILE_EXTENSION):
        train_dataset  = load_from_disk(filename_for_save + TRAIN_FILE_EXTENSION)
    else:
        train_dataset, _ = save_data_from_file(filename)
    return train_dataset

def get_testing_data(filename):
    """
        Loads and groups the test dataset from disk, organizing it by question.

        This function checks whether a preprocessed test dataset exists on disk.
        If it does, it loads the dataset and groups the examples by question.
        Each question maps to a list of tuples: (snippet, document, label).
    """
    filename_for_save = filename[len(DATA_FOLDER):].split(".")[0]
    if os.path.exists(filename_for_save + TEST_FILE_EXTENSION):
        test_dataset = load_from_disk(filename_for_save + TEST_FILE_EXTENSION)
        grouped_data = defaultdict(list)

        for example in test_dataset:
            question = example["question"]
            snippet = example["snippet"]
            label = example["label"]
            document = example["document"]
            grouped_data[question].append((snippet, document, label))
        return grouped_data


def process_data(path):
    """
        Parses a JSON file containing question and snippet data into grouped format.

        This function reads a JSON file expected to contain a list of questions, where
        each question includes a "body", an "id", and an optional list of "snippets".
        It groups the data by question text, collecting associated snippet information
        for each question.
    """
    with open(path) as f:
        data = json.load(f)

    question_groups = defaultdict(list)

    for q in data["questions"]:
        question = q["body"]
        q_id = q["id"]
        snippets = q.get("snippets", [])
        for i, snippet in enumerate(snippets):
            question_groups[question].append((
                snippet["text"],
                snippet["document"],
                len(snippets) - i,
                q_id,
                snippet
            ))
    return question_groups

def process_data_for_docs(path):
    """Parses a JSON file containing questions and their associated snippets, grouping the data by question."""
    with open(path) as f:
        data = json.load(f)

    question_groups = defaultdict(list)
    for q in data["questions"]:
        keys = [list(item.keys())[0] for item in q["extra_wuensche"]]
        question = q["body"]
        q_id = q["id"]
        docs = q["extra_wuensche"]
        for i, doc in enumerate(docs):
            question_groups[question].append((
                keys[i],
                doc[keys[i]],
                len(q) - i,
                q_id,
            ))
    return question_groups